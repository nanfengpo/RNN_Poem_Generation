{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import seq2seq\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poetry_file ='poetry.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class readPoems:\n",
    "    '''\n",
    "    Read Chinese poems and generate poem vectors for training\n",
    "    Args:\n",
    "    file_path:file path of Chinese poems\n",
    "    '''\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.poems = self._readfile()\n",
    "        \n",
    "    \n",
    "    \n",
    "    def _preProcessor(self, s):\n",
    "        #remove letters\n",
    "        s = re.sub('[a-zA-Z一《》]', '', s)\n",
    "        #remove blanks\n",
    "        s = re.sub(' ', '', s)\n",
    "        #remove digits\n",
    "        s = re.sub('['+string.digits+']', '', s)\n",
    "        #remove line ends\n",
    "        s = re.sub('\\n', ' ', s)\n",
    "        s = s.rstrip()\n",
    "        return s\n",
    "    \n",
    "    def _readfile(self):\n",
    "        poems = []\n",
    "        with open(self.file_path, \"r\", encoding='utf-8',) as fi:\n",
    "            for line in fi:\n",
    "                content = line.strip().split(':')[1]\n",
    "                if len(content) > 10:#skip short sentences\n",
    "                    poem = self._preProcessor(content)\n",
    "                    #Set beginning and ending for each poem\n",
    "                    poem ='(' + poem +')'\n",
    "                    poems.append(poem)\n",
    "                    \n",
    "        #Sort the poems according to their lengths\n",
    "        poems_lens = [len(poem) for poem in poems]\n",
    "        poem_tuple = list(zip(poems_lens, poems))\n",
    "        poem_tuple.sort()\n",
    "        poems = [item[1] for item in poem_tuple]\n",
    "        return poems\n",
    "    \n",
    "    def _poem2chars(self):\n",
    "        #Split each poem into Chinese characters\n",
    "        poems_chars= []\n",
    "        all_chars = []\n",
    "        for poem in self.poems:\n",
    "           #Skip too long or too short sentences\n",
    "           if len(poem) > 80 or len(poem)<5:\n",
    "              continue\n",
    "           if poem != ' ' or poem != None:\n",
    "              #Turn each poem into a list of vectors\n",
    "              poems_chars.append([char for char in poem])\n",
    "              #Collect all the chars\n",
    "              all_chars.extend([char for char in poem])\n",
    "        return poems_chars, all_chars\n",
    "    \n",
    "    def buildVocab(self):\n",
    "        _, all_chars = self._poem2chars()\n",
    "        #Calculate frequencies of each character\n",
    "        chars_freq = Counter(all_chars)\n",
    "        #Filter out those low frequency characters\n",
    "        vocab = [u for u,v in chars_freq.items() if v>10]\n",
    "        if ' ' not in vocab:\n",
    "            vocab.append(' ')\n",
    "        #Map each char into an ID\n",
    "        char_id_map = dict(zip(vocab, range(len(vocab))))\n",
    "        #Map each ID into a word\n",
    "        id_char_map = dict([(value, key) for key, value in char_id_map.items()])\n",
    "        return vocab, char_id_map, id_char_map\n",
    "    \n",
    "    def poem2vecs(self):\n",
    "        #Map each word into an ID\n",
    "        poems_chars, _ = self._poem2chars()\n",
    "        vocab, char_id_map, id_char_map = self.buildVocab()\n",
    "        def char2id(c):\n",
    "            try:\n",
    "               ID = char_id_map[c]\n",
    "            except:#Trun those less frequent words into blanks\n",
    "               ID = char_id_map[' ']\n",
    "            return ID\n",
    "        #Turn each poem into a list of word Ids\n",
    "        chars_vecs = lambda chars: [char2id(char) for char in chars]\n",
    "        poems_vecs = [chars_vecs(chars) for chars in poems_chars]\n",
    "        return poems_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poem_model = readPoems(poetry_file)\n",
    "vocab, char_id_map, id_char_map = poem_model.buildVocab()\n",
    "poems_vecs = poem_model.poem2vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[429, 1895, 2840, 1634, 1932, 764, 3647, 1915, 239, 1075, 100, 2831, 1380],\n",
       " [429, 1895, 1292, 2062, 3237, 3239, 764, 1807, 3352, 688, 775, 2831, 1380],\n",
       " [429, 1895, 2872, 2232, 1863, 3685, 764, 328, 1873, 3404, 2536, 2831, 1380],\n",
       " [429, 3061, 726, 3580, 985, 2035, 764, 54, 3188, 817, 2142, 2831, 1380],\n",
       " [429, 570, 688, 1520, 2937, 764, 3036, 1815, 405, 1793, 507, 2831, 1380],\n",
       " [429, 1964, 2826, 570, 1709, 1178, 764, 38, 2872, 23, 305, 2831, 1380],\n",
       " [429, 2848, 643, 1090, 315, 764, 2457, 2771, 661, 1482, 3422, 2831, 1380],\n",
       " [429, 726, 1025, 50, 83, 1880, 764, 3122, 2061, 3690, 2395, 2831, 1380],\n",
       " [429, 3181, 139, 2271, 2171, 2171, 764, 570, 2009, 1321, 2997, 2831, 1380],\n",
       " [429, 3181, 139, 2271, 2171, 2171, 764, 570, 2009, 1321, 2997, 2831, 1380]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poems_vecs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(万木无叶，客心悲此时。)',\n",
       " '(万般思后行，失废前功。)',\n",
       " '(万里岐路多，身天地窄。)',\n",
       " '(九子不葬父，女打荆棺。)',\n",
       " '(双前进士，两个阿孩儿。)',\n",
       " '(吟中双鬓白，笑里生贫。)',\n",
       " '(如蒙被服，方堪称福田。)',\n",
       " '(子母相去离，连台拗倒。)',\n",
       " '(宝帐香重重，双红芙蓉。)',\n",
       " '(宝帐香重重，双红芙蓉。)']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_model.poems[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def char2id(c):\n",
    "    try:\n",
    "        ID = char_id_map[c]\n",
    "    except:#Trun those less frequent words into blanks\n",
    "        ID = char_id_map[' ']\n",
    "    return ID\n",
    "      \n",
    "#turn an ID into a character\n",
    "def id2char(ID):\n",
    "    try:\n",
    "        char = id_char_map[ID]\n",
    "    except:\n",
    "        char = ' '\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "唐诗数量: 34921\n"
     ]
    }
   ],
   "source": [
    "print('唐诗数量:', len(poems_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3691"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAFkCAYAAACAUFlOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29ece1216d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "poems_lens = [len(chars) for chars in poems_vecs]\n",
    "plt.hist(poems_lens, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length: 13\n",
      "Max length: 80\n",
      "Average length: 49.4196615217\n",
      "Median length: 50.0\n"
     ]
    }
   ],
   "source": [
    "print('Min length:', np.amin(poems_lens))\n",
    "print('Max length:', np.amax(poems_lens))\n",
    "print('Average length:', np.mean(poems_lens))\n",
    "print('Median length:', np.median(poems_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13.,  34.,  50.,  65.,  74.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(poems_lens, [0, 25, 50, 75, 95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It seems more than 95% peoms has a length less than 167 characters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3690"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2id(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class generateSamples:\n",
    "    '''Generate samples for training'''\n",
    "    \n",
    "    def __init__(self, batch_size, poems_vecs):\n",
    "        '''Pass batch size and poems vectors'''\n",
    "        self.index = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.poems_vecs = poems_vecs\n",
    "        self.poem_count = len(poems_vecs)\n",
    "        self.max_len = max(map(len, poems_vecs))\n",
    "        \n",
    "    def generate_batch(self, same_length=False):\n",
    "        '''Generate a training sample each time'''\n",
    "        start = self.index\n",
    "        end = start + self.batch_size\n",
    "        #In case end goes beyong the range of the samples\n",
    "        if end > self.poem_count:\n",
    "            self.index = 0\n",
    "            start = 0\n",
    "            end = self.batch_size\n",
    "        data = self.poems_vecs[start: end]\n",
    "        #Set the max lengths as the size of the input\n",
    "        if same_length:\n",
    "            max_len = self.max_len\n",
    "        else:\n",
    "            max_len = max(map(len, data))\n",
    "        #Record lengths for each poem\n",
    "        lengths = [len(item) for item in data]\n",
    "        lengths = np.array(lengths)\n",
    "        self.index = (self.index + self.batch_size)%self.poem_count\n",
    "        #Create input and label\n",
    "        x = np.full((self.batch_size,max_len), char2id(' '), np.int32)\n",
    "        y = np.copy(x)\n",
    "        for i in range(self.batch_size):\n",
    "            #the first n elements as input\n",
    "            x[i, :(len(data[i])-1)] = data[i][:-1]\n",
    "            #the last n elements as labels\n",
    "            y[i, :(len(data[i])-1)] = data[i][1:]\n",
    "        return x, y, lengths, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate series of inputs and labels\n",
    "#Batch_size * lengths\n",
    "generatesamples = generateSamples(64, poems_vecs)\n",
    "x, y, lengths, _ = generatesamples.generate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_poem_len = generatesamples.max_len\n",
    "max_poem_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    vocab_size = len(vocab)\n",
    "    embed_size = 128\n",
    "    batch_size = 64\n",
    "    layer_size = 2\n",
    "    max_poem_len = max_poem_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_num = int(len(poems_vecs)/config.batch_size)\n",
    "chunk_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('placeholder'):\n",
    "        input_data = tf.placeholder(tf.int32, [config.batch_size, None])\n",
    "        output_targets = tf.placeholder(tf.int32, [config.batch_size, None])\n",
    "        lens = tf.placeholder(tf.int32, [config.batch_size])\n",
    "        #keep_prob_input = tf.placeholder(tf.float32)\n",
    "        #keep_prob_cell = tf.placeholder(tf.float32)\n",
    "    with tf.name_scope('Embedding'):\n",
    "        #Create embeddings for the characters\n",
    "        embeddings = tf.Variable(tf.random_uniform([config.vocab_size, \n",
    "                                                    config.embed_size], -1.0, 1.0))\n",
    "        inputs = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "        #Dropout\n",
    "        #inputs = tf.nn.dropout(inputs, keep_prob=0.7)\n",
    "\n",
    "    with tf.name_scope('RNNLayer') as scope:\n",
    "        #gru_cell = rnn.GRUCell(config.embed_size）\n",
    "        def lstm():\n",
    "            return rnn.BasicLSTMCell(config.embed_size, forget_bias=0.0, \n",
    "                                      state_is_tuple=True, reuse=tf.get_variable_scope().reuse) \n",
    "        lstm_cell = lstm\n",
    "        cell = rnn.MultiRNNCell([lstm_cell() for _ in range(config.layer_size)], \n",
    "                                state_is_tuple=True)\n",
    "        initial_state = cell.zero_state(config.batch_size, tf.float32)\n",
    "        #Dropout\n",
    "        #cell = rnn.DropoutWrapper(lstm_cell, output_keep_prob=0.7)\n",
    "        weights = tf.Variable(tf.random_normal([config.embed_size, config.vocab_size]), name='weight')\n",
    "        biases = tf.Variable(tf.random_normal([config.vocab_size]), name='bias')\n",
    "        #outputs, status = rnn.static_rnn(lstm, inputs, sequence_length=lens)\n",
    "        #Dynamic RNN with given lengths for each poem\n",
    "        #status = initial_state\n",
    "        inputs = tf.nn.dropout(inputs, 0.8)\n",
    "        outputs, status = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state,\n",
    "                                            sequence_length=lens, dtype=tf.float32, \n",
    "                                            scope='RNNLayer')\n",
    "        output = tf.reshape(outputs,[-1, config.embed_size])\n",
    "        logits = tf.matmul(output, weights) + biases\n",
    "        #logits = tf.contrib.layers.fully_connected(output, config.vocab_size, activation_fn=None)\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        \n",
    "\n",
    "    with tf.name_scope('Optimizer'):\n",
    "        targets = tf.reshape(output_targets, [-1])\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "            [logits],\n",
    "            [targets],\n",
    "            [tf.ones_like(targets, dtype=tf.float32)], config.vocab_size)\n",
    "        cost = tf.reduce_mean(loss) \n",
    "        learning_rate = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), 5)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    \n",
    "    with tf.name_scope('Validation'):\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        #Define initial state for validation set\n",
    "        initial_sample_state = cell.zero_state(1, tf.float32)\n",
    "        sample_input = tf.placeholder(tf.int32, shape=[1, 1])\n",
    "        sample_emd = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "        state_sample = initial_sample_state\n",
    "        output_sample, status_sample = tf.nn.dynamic_rnn(cell, sample_emd, initial_state=state_sample,\n",
    "                                             dtype=tf.float32, scope='RNNLayer')\n",
    "        output_sample = tf.reshape(output_sample,[-1, config.embed_size])\n",
    "        logits_sample = tf.matmul(output_sample, weights) + biases\n",
    "        #logits_sample = tf.contrib.layers.fully_connected(output_sample, config.vocab_size, activation_fn=None)\n",
    "        probs_sample= tf.nn.softmax(logits_sample)\n",
    "        prediction = tf.argmax(probs_sample, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.5702\n",
      "================================================================================\n",
      "(仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇仇\n",
      "================================================================================\n",
      "Loss: 6.1437\n",
      "Loss: 6.8374\n",
      "================================================================================\n",
      "(不知不知不知年。)\n",
      "================================================================================\n",
      "Loss: 5.6801\n",
      "Loss: 5.9747\n",
      "================================================================================\n",
      "(不知不知何处处，不知何处不知人。)\n",
      "================================================================================\n",
      "Loss: 5.5316\n",
      "Loss: 5.2316\n",
      "================================================================================\n",
      "(春风不可见，春风落日来。)\n",
      "================================================================================\n",
      "Loss: 5.7196\n",
      "Loss: 6.5121\n",
      "================================================================================\n",
      "(不知何处不知。)\n",
      "================================================================================\n",
      "Loss: 5.2609\n",
      "Loss: 5.475\n",
      "================================================================================\n",
      "(不知春水不知春，不知春水不堪情。不知不得无人事，不知何处不知人。)\n",
      "================================================================================\n",
      "Loss: 5.1457\n",
      "Loss: 4.8361\n",
      "================================================================================\n",
      "(春风吹落叶，寒风落日开。水流秋色满，山色不相逢。)\n",
      "================================================================================\n",
      "Loss: 5.2803\n",
      "Loss: 6.0457\n",
      "================================================================================\n",
      "(江南山下水流流，不见三年不得知。不知此地无人事，不是人间不可怜。不知此地无人事，不是人间不可怜。)\n",
      "================================================================================\n",
      "Loss: 4.9414\n",
      "Loss: 5.2007\n",
      "================================================================================\n",
      "(    不得归，不知何处是何人。今朝不得无人事，不见春风不可怜。)\n",
      "================================================================================\n",
      "Loss: 4.887\n",
      "Loss: 4.6127\n",
      "================================================================================\n",
      "(春风吹落叶，春色满山阴。水色无人事，春风满地深。不知春草色，不是故人同。)\n",
      "================================================================================\n",
      "Loss: 5.0337\n",
      "Loss: 5.8426\n",
      "================================================================================\n",
      "(白发无人不可知，不知何处是谁家。山中不见人间事，不是人间不可知。不是不知何处去，不知何处是谁家。)\n",
      "================================================================================\n",
      "Loss: 4.7752\n",
      "Loss: 5.0283\n",
      "================================================================================\n",
      "(白发无人不得归，不知何处是无人。)\n",
      "================================================================================\n",
      "Loss: 4.7709\n",
      "Loss: 4.504\n",
      "================================================================================\n",
      "(春风吹落叶，春色满庭枝。日暮江南树，春风满地深。春风吹落日，春色满庭枝。)\n",
      "================================================================================\n",
      "Loss: 4.884\n",
      "Loss: 5.6557\n",
      "================================================================================\n",
      "(白云何处是谁家，白发何人更有情。不是不知何处去，不知何处是谁家。山川不见春风起，月照江湖水色长。不是\n",
      "================================================================================\n",
      "Loss: 4.6777\n",
      "Loss: 4.9281\n",
      "================================================================================\n",
      "(白云何处不堪闻，白发无人不见人。)\n",
      "================================================================================\n",
      "Loss: 4.6811\n",
      "Loss: 4.4289\n",
      "================================================================================\n",
      "(春风吹落日，春色满山阴。水色无人语，山川日日斜。春风吹落日，春色满山阴。不是长安路，无人不可知。)\n",
      "================================================================================\n",
      "Loss: 4.7683\n",
      "Loss: 5.4578\n",
      "================================================================================\n",
      "(白云何处是谁家，白发无人不可知。山水不知何处去，山川不见水流流。风吹不似江山月，月照江湖水绕天。莫道\n",
      "================================================================================\n",
      "Loss: 4.5741\n",
      "Loss: 4.8361\n",
      "================================================================================\n",
      "(白云飞去不堪闻，白发无人不见人。今日相思不相见，不知何处是无穷。)\n",
      "================================================================================\n",
      "Loss: 4.6143\n",
      "Loss: 4.3834\n",
      "================================================================================\n",
      "(春风吹落叶，日日照秋风。日暮春风起，春风满路长。风流千里外，日日见春风。不是长安客，无人不可知。)\n",
      "================================================================================\n",
      "Loss: 4.7057\n",
      "Loss: 5.3443\n",
      "================================================================================\n",
      "(白云何处是长安，白发无人不可寻。风雨不知山上客，月明山色见江湖。山川不见春风起，月下江南雁过时。不是\n",
      "================================================================================\n",
      "Loss: 4.4954\n",
      "Loss: 4.7468\n",
      "================================================================================\n",
      "(白云飞去不堪闻，不见人间不得归。莫怪不知何处去，不知何处是长安。)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for l in range(epochs):\n",
    "        for i in range(chunk_num):\n",
    "            sess.run(tf.assign(learning_rate, 0.003*((0.98)**l)))\n",
    "            x, y, lengths, _ = generatesamples.generate_batch()\n",
    "            feed_dict = {input_data:x, output_targets:y, lens:lengths}\n",
    "            l, _, _ = sess.run([cost, status, train_op], feed_dict=feed_dict)\n",
    "            if i%100 == 0:\n",
    "                print('Loss:', round(l, 4))\n",
    "            if i % 200 == 0:\n",
    "                print('=' * 80)\n",
    "                feed = np.zeros([1, 1])\n",
    "                feed[0, 0] = char2id('(')\n",
    "                s = id2char(feed[0][0])\n",
    "                #Initialize the original state\n",
    "                state_ = sess.run(initial_sample_state)\n",
    "                [pred_id, state_] = sess.run([prediction, status_sample], feed_dict={sample_input: feed, \n",
    "                                                                          state_sample: state_})\n",
    "                #Make current prediction as next input\n",
    "                #Use current state as next initialstate\n",
    "                for _ in range(50):\n",
    "                    s += id2char(pred_id[0])\n",
    "                    #Set '(' as beginning and ')' as ending\n",
    "                    if id2char(pred_id[0]) == ')':\n",
    "                        break\n",
    "                    feed[0, 0] = pred_id[0]\n",
    "                    [pred_id, state_] = sess.run([prediction, status_sample], \n",
    "                                                 feed_dict={sample_input: feed, \n",
    "                                                            state_sample: state_})\n",
    "                print(s)\n",
    "                print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def lazy_property(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class testConfig:\n",
    "    vocab_size = len(vocab)\n",
    "    embed_size = 128\n",
    "    batch_size = 1\n",
    "    layer_size = 2\n",
    "    max_poem_len = max_poem_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, in order to share parameters such as weights and biases in the architecture, it is necessary to specify \"variable_scope\" and use \"tf.get_variable\" method to share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define a RNN model \n",
    "class rnnModel:\n",
    "    '''Define a basic model for character classification, the model\n",
    "    Provides graph structure of tensorflow'''\n",
    "    \n",
    "    def __init__(self, x, y, lengths, config, is_training=True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.lengths = lengths\n",
    "        self.is_training = is_training\n",
    "        self.batch_size = config.batch_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embed_size = config.embed_size\n",
    "        self._learning_rate = None\n",
    "        self._initial_state = None\n",
    "        self._final_state = None\n",
    "        self.prediction\n",
    "        if not is_training:\n",
    "            print('Initializing Testing Model！')\n",
    "            return\n",
    "        self.cost\n",
    "        self.optimize\n",
    "        print('Initializing Training Model!') \n",
    "    \n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        #定义权重和偏置参数变量\n",
    "        logits= self.rnnLayer\n",
    "        targets = tf.reshape(self.y, [-1])\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "            [logits],\n",
    "            [targets],\n",
    "            [tf.ones_like(targets, dtype=tf.float32)])\n",
    "        cost = tf.reduce_mean(loss) \n",
    "        return cost\n",
    "\n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        cost = self.cost\n",
    "        #with tf.name_scope('Optimizer'):\n",
    "        self._learning_rate = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), 5)\n",
    "        optimizer = tf.train.AdamOptimizer(self._learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        return train_op\n",
    "\n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        logits= self.rnnLayer\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        prediction = tf.argmax(probs, 1)\n",
    "        return prediction\n",
    "    \n",
    "    @lazy_property\n",
    "    def rnnLayer(self):\n",
    "        #Create two layers rnn\n",
    "        def lstm():\n",
    "            return rnn.BasicLSTMCell(self.embed_size, forget_bias=0.0, \n",
    "                                      state_is_tuple=True, reuse=tf.get_variable_scope().reuse) \n",
    "        lstm_cell = lstm\n",
    "        #if self.is_training:\n",
    "           #def lstm_cell():\n",
    "                #return tf.contrib.rnn.DropoutWrapper(lstm(), output_keep_prob=0.8)\n",
    "        cell = rnn.MultiRNNCell([lstm_cell() for _ in range(2)], \n",
    "                                    state_is_tuple=True)\n",
    "            \n",
    "        self._initial_state = cell.zero_state(self.batch_size, tf.float32)\n",
    "        #Note we need get_variable method to share weights between objects\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embeddings = tf.get_variable('embedding', [self.vocab_size,  self.embed_size])\n",
    "            inputs = tf.nn.embedding_lookup(embeddings, self.x)\n",
    "        lens = tf.constant(np.ones(1))\n",
    "        #if self.is_training:\n",
    "            #inputs = tf.nn.dropout(inputs, 0.6)\n",
    "            #lens = self.lengths\n",
    "        state = self._initial_state\n",
    "        with tf.variable_scope('RNN'):\n",
    "            outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, \n",
    "                                                 initial_state=state,\n",
    "                                                #sequence_length=lens,\n",
    "                                                 dtype=tf.float32, scope='RNN')\n",
    "        self._final_state = final_state\n",
    "        output = tf.reshape(outputs,[-1, self.embed_size])\n",
    "        #self.weights = weights = tf.Variable(tf.random_normal([self.embed_size, config.vocab_size]), name='weight')\n",
    "        #biases = tf.Variable(tf.random_normal([self.vocab_size]), name='bias')\n",
    "        weights = tf.get_variable('weights', [self.embed_size, config.vocab_size], dtype=tf.float32)\n",
    "        biases = tf.get_variable('biases', [config.vocab_size], dtype=tf.float32)\n",
    "        logits = tf.matmul(output, weights) + biases\n",
    "        #logits = tf.contrib.layers.fully_connected(output, self.vocab_size, activation_fn=None)\n",
    "        return logits\n",
    "    \n",
    "    @property\n",
    "    def learningRate(self):\n",
    "        return self._learning_rate\n",
    "    \n",
    "    @property\n",
    "    def initialState(self):\n",
    "        return self._initial_state\n",
    "    \n",
    "    @property\n",
    "    def finalState(self):\n",
    "        return self._final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Training Model!\n",
      "Initializing Testing Model！\n",
      "Initializing Testing Model！\n"
     ]
    }
   ],
   "source": [
    "graph3 = tf.Graph()\n",
    "with graph3.as_default():\n",
    "    initializer = tf.random_uniform_initializer(-0.2, 0.2)\n",
    "    with tf.name_scope('Train'):\n",
    "        input_data = tf.placeholder(tf.int32, [config.batch_size, None])\n",
    "        output_targets = tf.placeholder(tf.int32, [config.batch_size, None])\n",
    "        lens = tf.placeholder(tf.int32, [config.batch_size])\n",
    "        with tf.variable_scope(\"Model\", reuse=None,initializer=initializer):\n",
    "            train_model = rnnModel(input_data, output_targets, lens, config)\n",
    "            \n",
    "    with tf.name_scope('Valid'):\n",
    "        valid_data = tf.placeholder(tf.int32, [config.batch_size, None])\n",
    "        valid_targets = tf.placeholder(tf.int32, [config.batch_size, None])\n",
    "        valid_lens = tf.placeholder(tf.int32, [config.batch_size])\n",
    "        with tf.variable_scope(\"Model\", reuse=True):\n",
    "            valid_model = rnnModel(valid_data, valid_targets, valid_lens, config, is_training=False)\n",
    "            \n",
    "    with tf.name_scope('Test'):\n",
    "        sample_data = tf.placeholder(tf.int32, [1, 1])\n",
    "        sample_len = tf.placeholder(tf.int32, [1])\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            test_model = rnnModel(x=sample_data, y=None, lengths=sample_len, config=testConfig, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.2395\n",
      "(香荒愈愈愈夷俱恨恨汶罾罾等等劚慢慢铿铿镌臼奔奔敬镌鸥瞳瞳瞳寨请请请曜芙乖乖乖仪仪葭葭守守如螭等请请请\n",
      "================================================================================\n",
      "Loss: 6.0486\n",
      "Loss: 5.576\n",
      "Loss: 5.6784\n",
      "Loss: 5.5052\n",
      "(山山山，不人山人。)\n",
      "================================================================================\n",
      "Loss: 5.5468\n",
      "Loss: 5.3934\n",
      "Loss: 5.3163\n",
      "Loss: 5.8321\n",
      "(不知不有不得，不知不有不知。)\n",
      "================================================================================\n",
      "Loss: 5.5796\n",
      "Loss: 5.3922\n",
      "Loss: 5.0863\n",
      "(春日   ，天人不可知。)\n",
      "================================================================================\n",
      "Loss: 5.5259\n",
      "Loss: 4.9329\n",
      "Loss: 5.1019\n",
      "Loss: 5.0249\n",
      "(山山日日，山山不可。不知何处，何事不相知。)\n",
      "================================================================================\n",
      "Loss: 5.1183\n",
      "Loss: 4.9311\n",
      "Loss: 4.8759\n",
      "Loss: 5.429\n",
      "(山上春风满，春风不见春。风风不见春，月下青云不可。不知不得人，不是人间不可知。)\n",
      "================================================================================\n",
      "Loss: 5.1764\n",
      "Loss: 5.0716\n",
      "Loss: 5.0369\n",
      "(天里无人事，天中不可知。)\n",
      "================================================================================\n",
      "Loss: 5.1784\n",
      "Loss: 4.6875\n",
      "Loss: 4.8659\n",
      "Loss: 4.7898\n",
      "(山上无人，何人不可然。不知何处，何事是何人。不知何处，何事是何人。)\n",
      "================================================================================\n",
      "Loss: 4.9359\n",
      "Loss: 4.7663\n",
      "Loss: 4.714\n",
      "Loss: 5.1986\n",
      "(高台不得不知人，白首无人不可知。风吹不觉风花落，月上江山月满山。风吹水上风花落，月上山山月上春。不知\n",
      "================================================================================\n",
      "Loss: 4.9989\n",
      "Loss: 4.9025\n",
      "Loss: 4.8747\n",
      "(     ，     。                                      \n",
      "================================================================================\n",
      "Loss: 4.9766\n",
      "Loss: 4.5253\n",
      "Loss: 4.701\n",
      "Loss: 4.6209\n",
      "(白发无穷事，无人不可然。不知何处，何事不知。不见山中去，何人不可知。)\n",
      "================================================================================\n",
      "Loss: 4.7903\n",
      "Loss: 4.6236\n",
      "Loss: 4.5688\n",
      "Loss: 5.0236\n",
      "(高台不得归人别，白发无人不得知。不知此地无人事，不得人间不得知。不知此地无人事，不得东山不得知。)\n",
      "================================================================================\n",
      "Loss: 4.8755\n",
      "Loss: 4.7813\n",
      "Loss: 4.7415\n",
      "(白云生不得，不见此中同。日日无人事，无人不可知。风光初不见，风雨满山空。日暮春风起，风吹水色新。风光\n",
      "================================================================================\n",
      "Loss: 4.843\n",
      "Loss: 4.3962\n",
      "Loss: 4.5833\n",
      "Loss: 4.5002\n",
      "(白云无限，不是此中来。不见长安客，无人不可怜。何人不得，不是此中人。何事无穷事，何人不可知。)\n",
      "================================================================================\n",
      "Loss: 4.6806\n",
      "Loss: 4.5062\n",
      "Loss: 4.4525\n",
      "Loss: 4.8753\n",
      "(高楼不见春风起，白发无人不可知。不是故人无事事，不知何处是谁家。山中不得无人事，山下无人不见君。不是\n",
      "================================================================================\n",
      "Loss: 4.7734\n",
      "Loss: 4.6811\n",
      "Loss: 4.524\n",
      "(白日生新日，清风入翠微。风光生玉树，玉辇出金门。日照风光动，风摇水色新。风光生玉树，风雨入金墀。日照\n",
      "================================================================================\n",
      "Loss: 4.7078\n",
      "Loss: 4.3019\n",
      "Loss: 4.4761\n",
      "Loss: 4.4205\n",
      "(白云无限，不是此中来。不见人间事，何人不可知。何人不得，不敢是何人。)\n",
      "================================================================================\n",
      "Loss: 4.5919\n",
      "Loss: 4.4154\n",
      "Loss: 4.3641\n",
      "Loss: 4.7832\n",
      "(高楼不见白云时，白发无人不可知。不是故人无事，不知何处是谁家。山中不是无人事，山下无人问故人。)\n",
      "================================================================================\n",
      "Loss: 4.6949\n",
      "Loss: 4.6018\n",
      "Loss: 4.3622\n",
      "(白日生新日，清风入玉门。风光初照日，风雨满金闺。日日风光动，春风入翠微。风光生玉树，花影落花枝。日照\n",
      "================================================================================\n",
      "Loss: 4.6196\n",
      "Loss: 4.2207\n",
      "Loss: 4.3815\n",
      "Loss: 4.352\n",
      "(白云何处，千年万里来。不知何处去，不觉旧时闻。不见人间事，何人问故乡。)\n",
      "================================================================================\n",
      "Loss: 4.5141\n",
      "Loss: 4.3419\n",
      "Loss: 4.2984\n",
      "Loss: 4.7052\n",
      "(高楼不见白云时，白发无人不可知。不是故人无事事，不知何处是天台。山中不见人间事，山下无人问故乡。不是\n",
      "================================================================================\n",
      "Loss: 4.626\n",
      "Loss: 4.5397\n",
      "Loss: 4.2165\n",
      "(白日生新日，春光入御沟。     ，     。       ，      。       ，   \n",
      "================================================================================\n",
      "Loss: 4.53\n",
      "Loss: 4.1575\n",
      "Loss: 4.2989\n",
      "Loss: 4.2957\n",
      "(白云何处，千年万里。不知何处去，何事不相逢。不见春风起，空闻白发生。何当此身在，不得到何人。)\n",
      "================================================================================\n",
      "Loss: 4.4535\n",
      "Loss: 4.2804\n",
      "Loss: 4.2393\n",
      "Loss: 4.6429\n",
      "(江南万里春风起，日暮风流不可寻。山下不知何处去，山中无事不知时。山中不见人间事，山下无人问故乡。不是\n",
      "================================================================================\n",
      "Loss: 4.5773\n",
      "Loss: 4.4879\n",
      "Loss: 4.092\n",
      "(白日何年别，东风入旧林。不知春草合，不见白云生。)\n",
      "================================================================================\n",
      "Loss: 4.4593\n",
      "Loss: 4.1045\n",
      "Loss: 4.2268\n",
      "Loss: 4.2432\n",
      "(白发何年别，孤舟不可寻。不知何处去，不觉旧山来。不见人间事，何人问故乡。)\n",
      "================================================================================\n",
      "Loss: 4.4058\n",
      "Loss: 4.2301\n",
      "Loss: 4.193\n",
      "Loss: 4.5984\n",
      "(江南万里春风起，日暮风流不可寻。山下不知何处去，山中无事不知春。山中有客无人语，山下无人见故乡。不是\n",
      "================================================================================\n",
      "Loss: 4.5397\n",
      "Loss: 4.4456\n",
      "Loss: 4.049\n",
      "(白日东西去，东风入楚山。云深山色晚，月照水门秋。白日无人识，山川不可寻。山中有馀景，山下无人识。)\n",
      "================================================================================\n",
      "Loss: 4.3906\n",
      "Loss: 4.0705\n",
      "Loss: 4.1642\n",
      "Loss: 4.1969\n",
      "(白发何时见，相逢不可寻。不知何处去，不觉更无人。不见山中客，何人问故乡。何当此身在，何处是吾师。)\n",
      "================================================================================\n",
      "Loss: 4.363\n",
      "Loss: 4.187\n",
      "Loss: 4.1468\n",
      "Loss: 4.569\n",
      "(江南万里春风起，白日东西日日曛。山下水声连雨夜，山中山色夜相思。江村日暮无人语，风雨萧萧不见春。)\n",
      "================================================================================\n",
      "Loss: 4.5024\n",
      "Loss: 4.4128\n",
      "Loss: 3.9614\n",
      "(白日东西去，东风白日新。不知山水远，不见白云人。)\n",
      "================================================================================\n",
      "Loss: 4.3321\n",
      "Loss: 4.0171\n",
      "Loss: 4.1172\n",
      "Loss: 4.1667\n",
      "(白发何年别，相逢不可寻。不知何处去，何处是闲人。不见山中客，何人问故乡。)\n",
      "================================================================================\n",
      "Loss: 4.3312\n",
      "Loss: 4.1501\n",
      "Loss: 4.1088\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "states = []\n",
    "with tf.Session(graph=graph3) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for l in range(epochs):\n",
    "        sess.run(tf.assign(train_model.learningRate, 0.004*(0.97**l)))\n",
    "        for i in range(chunk_num):\n",
    "            #Assign a learning rate\n",
    "            #sess.run(tf.assign(model.isTraining, True))\n",
    "            x, y, lengths, _ = generatesamples.generate_batch()\n",
    "            feed_dict = {input_data:x, output_targets:y, lens:lengths}\n",
    "            l, _ = sess.run([train_model.cost, train_model.optimize], feed_dict=feed_dict)\n",
    "            if i%50 == 0:\n",
    "                print('Loss:', round(l, 4))\n",
    "            if i%200 == 0:\n",
    "\n",
    "                feed = np.zeros([1, 1])\n",
    "                feed[0, 0] = char2id('(')\n",
    "                s = '('\n",
    "\n",
    "                #Specify initial state\n",
    "                #Initialize the state of testing data\n",
    "                initial_state = sess.run(test_model.initialState)\n",
    "                state = initial_state\n",
    "                #Record current final state and make it as next inital state\n",
    "                #s = id2char(pred_id[0][0])\n",
    "                for _ in range(50):\n",
    "                    fd= {sample_data:feed, test_model.initialState:state}\n",
    "                    state, pred_id = sess.run([test_model.finalState, \n",
    "                                                     test_model.prediction], feed_dict=fd)\n",
    "                    feed[0, 0] = pred_id[0]\n",
    "                    s += id2char(pred_id[0])\n",
    "                    if id2char(pred_id[0]) == ')':\n",
    "                        break\n",
    "                print(s)\n",
    "                print('='*80)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
