{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import seq2seq\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poetry_file ='poetry.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class readPoems:\n",
    "    '''\n",
    "    Read Chinese poems and generate poem vectors for training\n",
    "    Args:\n",
    "    file_path:file path of Chinese poems\n",
    "    '''\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.poems = self._readfile()\n",
    "        self.poems_chars, self.all_chars = self._poem2chars()\n",
    "    \n",
    "    \n",
    "    def _preProcessor(self, s):\n",
    "        #remove letters\n",
    "        s = re.sub('[a-zA-Z一《》]', '', s)\n",
    "        #remove blanks\n",
    "        s = re.sub(' ', '', s)\n",
    "        #remove digits\n",
    "        s = re.sub('['+string.digits+']', '', s)\n",
    "        #remove line ends\n",
    "        s = re.sub('\\n', ' ', s)\n",
    "        s = s.rstrip()\n",
    "        return s\n",
    "    \n",
    "    def _readfile(self):\n",
    "        poems = []\n",
    "        with open(self.file_path, \"r\", encoding='utf-8',) as fi:\n",
    "            for line in fi:\n",
    "                content = line.strip().split(':')[1]\n",
    "                if len(content) > 10:#skip short sentences\n",
    "                    poem = self._preProcessor(content)\n",
    "                    #Set beginning and ending for each poem\n",
    "                    poem ='(' + poem +')'\n",
    "                    poems.append(poem)\n",
    "                    \n",
    "        #Sort the poems according to their lengths\n",
    "        poems_lens = [len(poem) for poem in poems]\n",
    "        poem_tuple = list(zip(poems_lens, poems))\n",
    "        poem_tuple.sort()\n",
    "        poems = [item[1] for item in poem_tuple]\n",
    "        return poems\n",
    "    \n",
    "    def _poem2chars(self):\n",
    "        #Split each poem into Chinese characters\n",
    "        poems_chars= []\n",
    "        all_chars = []\n",
    "        for poem in self.poems:\n",
    "           #Skip too long or too short sentences\n",
    "           if len(poem) > 80 or len(poem)<5:\n",
    "              continue\n",
    "           if poem != ' ' or poem != None:\n",
    "              #Turn each poem into a list of vectors\n",
    "              poems_chars.append([char for char in poem])\n",
    "              #Collect all the chars\n",
    "              all_chars.extend([char for char in poem])\n",
    "        return poems_chars, all_chars\n",
    "    \n",
    "    def buildVocab(self):\n",
    "        #_, all_chars = self._poem2chars()\n",
    "        #Calculate frequencies of each character\n",
    "        chars_freq = Counter(self.all_chars)\n",
    "        #Filter out those low frequency characters\n",
    "        vocab = [u for u,v in chars_freq.items() if v>10]\n",
    "        if ' ' not in vocab:\n",
    "            vocab.append(' ')\n",
    "        #Map each char into an ID\n",
    "        char_id_map = dict(zip(vocab, range(len(vocab))))\n",
    "        #Map each ID into a word\n",
    "        id_char_map = dict([(value, key) for key, value in char_id_map.items()])\n",
    "        return vocab, char_id_map, id_char_map\n",
    "    \n",
    "    def poem2vecs(self):\n",
    "        #Map each word into an ID\n",
    "        #poems_chars, _ = self._poem2chars()\n",
    "        poems_chars = self.poems_chars\n",
    "        vocab, char_id_map, id_char_map = self.buildVocab()\n",
    "        def char2id(c):\n",
    "            try:\n",
    "               ID = char_id_map[c]\n",
    "            except:#Trun those less frequent words into blanks\n",
    "               ID = char_id_map[' ']\n",
    "            return ID\n",
    "        #Turn each poem into a list of word Ids\n",
    "        chars_vecs = lambda chars: [char2id(char) for char in chars]\n",
    "        poems_vecs = [chars_vecs(chars) for chars in poems_chars]\n",
    "        return poems_vecs\n",
    "    \n",
    "    def flatten_poems(self):\n",
    "        #_, all_chars = self._poem2chars()\n",
    "        all_chars = self.all_chars\n",
    "        _, char_id_map, _ = self.buildVocab()\n",
    "        def char2id(c):\n",
    "            try:\n",
    "               ID = char_id_map[c]\n",
    "            except:#Trun those less frequent words into blanks\n",
    "               ID = char_id_map[' ']\n",
    "            return ID\n",
    "        flatten_poems_vecs = [char2id(c) for c in all_chars]\n",
    "        return flatten_poems_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poem_model = readPoems(poetry_file)\n",
    "vocab, char_id_map, id_char_map = poem_model.buildVocab()\n",
    "poems_vecs = poem_model.poem2vecs()\n",
    "flatten_poems_vecs = poem_model.flatten_poems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(万木无叶，客心悲此时。)',\n",
       " '(万般思后行，失废前功。)',\n",
       " '(万里岐路多，身天地窄。)',\n",
       " '(九子不葬父，女打荆棺。)',\n",
       " '(双前进士，两个阿孩儿。)',\n",
       " '(吟中双鬓白，笑里生贫。)',\n",
       " '(如蒙被服，方堪称福田。)',\n",
       " '(子母相去离，连台拗倒。)',\n",
       " '(宝帐香重重，双红芙蓉。)',\n",
       " '(宝帐香重重，双红芙蓉。)']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_model.poems[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def char2id(c):\n",
    "    try:\n",
    "        ID = char_id_map[c]\n",
    "    except:#Trun those less frequent words into blanks\n",
    "        ID = char_id_map[' ']\n",
    "    return ID\n",
    "      \n",
    "#turn an ID into a character\n",
    "def id2char(ID):\n",
    "    try:\n",
    "        char = id_char_map[ID]\n",
    "    except:\n",
    "        char = ' '\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "唐诗数量: 34921\n"
     ]
    }
   ],
   "source": [
    "print('唐诗数量:', len(poems_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3691"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11483f748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "poems_lens = [len(chars) for chars in poems_vecs]\n",
    "plt.hist(poems_lens, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length: 13\n",
      "Max length: 80\n",
      "Average length: 49.4196615217\n",
      "Median length: 50.0\n"
     ]
    }
   ],
   "source": [
    "print('Min length:', np.amin(poems_lens))\n",
    "print('Max length:', np.amax(poems_lens))\n",
    "print('Average length:', np.mean(poems_lens))\n",
    "print('Median length:', np.median(poems_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13.,  34.,  50.,  65.,  74.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(poems_lens, [0, 25, 50, 75, 95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It seems more than 95% peoms has a length less than 167 characters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3690"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2id(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class generateSamples:\n",
    "    '''Generate samples for training'''\n",
    "    \n",
    "    def __init__(self, batch_size, poems_vecs):\n",
    "        '''Pass batch size and poems vectors'''\n",
    "        self.poem_index = 0\n",
    "        self.char_index = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.poems_vecs = poems_vecs\n",
    "        self.poem_count = len(poems_vecs)\n",
    "        self.max_len = max(map(len, poems_vecs))\n",
    "        self.chars = self._get_chars()\n",
    "        \n",
    "    def generate_batch(self, same_length=False):\n",
    "        '''Generate a training sample containing a complete \n",
    "        poem each time'''\n",
    "        start = self.poem_index\n",
    "        end = start + self.batch_size\n",
    "        #In case end goes beyong the range of the samples\n",
    "        if end > self.poem_count:\n",
    "            self.poem_index = 0\n",
    "            start = 0\n",
    "            end = self.batch_size\n",
    "        data = self.poems_vecs[start: end]\n",
    "        #Set the max lengths as the size of the input\n",
    "        if same_length:\n",
    "            max_len = self.max_len\n",
    "        else:\n",
    "            max_len = max(map(len, data))\n",
    "        #Record lengths for each poem\n",
    "        lengths = [len(item) for item in data]\n",
    "        lengths = np.array(lengths)\n",
    "        self.poem_index = (self.poem_index + self.batch_size)%self.poem_count\n",
    "        #Create input and label\n",
    "        x = np.full((self.batch_size,max_len), char2id(' '), np.int32)\n",
    "        y = np.copy(x)\n",
    "        for i in range(self.batch_size):\n",
    "            #the first n elements as input\n",
    "            x[i, :(len(data[i])-1)] = data[i][:-1]\n",
    "            #the last n elements as labels\n",
    "            y[i, :(len(data[i])-1)] = data[i][1:]\n",
    "        return x, y, lengths, max_len\n",
    "    \n",
    "    def _get_chars(self):\n",
    "        peoms_vecs_flatten = []\n",
    "        for items in self.poems_vecs:\n",
    "            peoms_vecs_flatten.extend(items)\n",
    "        data_len = len(peoms_vecs_flatten)\n",
    "        batch_len = data_len // self.batch_size\n",
    "        data = np.reshape(peoms_vecs_flatten[0: self.batch_size * batch_len],\n",
    "                              [self.batch_size, batch_len])\n",
    "        return data\n",
    "    \n",
    "    def generate_batch_same_size(self, num_steps):\n",
    "        '''\n",
    "        Generate a batch size of characters within all the poems.\n",
    "        Put all the poems into a long sentence.\n",
    "        '''\n",
    "        data = self.chars\n",
    "        data_len = len(data)\n",
    "        start = self.char_index\n",
    "        end = self.char_index + num_steps\n",
    "        if end > (data_len-1):\n",
    "            self.char_index = 0\n",
    "            start = 0\n",
    "            end = num_steps\n",
    "        self.char_index += num_steps\n",
    "        x = data[:, start:end]\n",
    "        y = data[:, (start+1):(end+1)]\n",
    "        return x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate series of inputs and labels\n",
    "#Batch_size * lengths\n",
    "generatesamples = generateSamples(64, poems_vecs)\n",
    "x, y, lengths, _ = generatesamples.generate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_poem_len = generatesamples.max_len\n",
    "max_poem_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    vocab_size = len(vocab)\n",
    "    embed_size = 128\n",
    "    batch_size = 64\n",
    "    layer_size = 2\n",
    "    max_poem_len = max_poem_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_num = int(len(poems_vecs)/config.batch_size)\n",
    "chunk_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('placeholder'):\n",
    "        input_data = tf.placeholder(tf.int32, [config.batch_size, None])\n",
    "        output_targets = tf.placeholder(tf.int32, [config.batch_size, None])\n",
    "        lens = tf.placeholder(tf.int32, [config.batch_size])\n",
    "        #keep_prob_input = tf.placeholder(tf.float32)\n",
    "        #keep_prob_cell = tf.placeholder(tf.float32)\n",
    "    with tf.name_scope('Embedding'):\n",
    "        #Create embeddings for the characters\n",
    "        embeddings = tf.Variable(tf.random_uniform([config.vocab_size, \n",
    "                                                    config.embed_size], -1.0, 1.0))\n",
    "        inputs = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "        #Dropout\n",
    "        #inputs = tf.nn.dropout(inputs, keep_prob=0.7)\n",
    "\n",
    "    with tf.name_scope('RNNLayer') as scope:\n",
    "        #gru_cell = rnn.GRUCell(config.embed_size）\n",
    "        def lstm():\n",
    "            return rnn.BasicLSTMCell(config.embed_size, forget_bias=0.0, \n",
    "                                      state_is_tuple=True, reuse=tf.get_variable_scope().reuse) \n",
    "        lstm_cell = lstm\n",
    "        cell = rnn.MultiRNNCell([lstm_cell() for _ in range(config.layer_size)], \n",
    "                                state_is_tuple=True)\n",
    "        initial_state = cell.zero_state(config.batch_size, tf.float32)\n",
    "        #Dropout\n",
    "        #cell = rnn.DropoutWrapper(lstm_cell, output_keep_prob=0.7)\n",
    "        weights = tf.Variable(tf.random_normal([config.embed_size, config.vocab_size]), name='weight')\n",
    "        biases = tf.Variable(tf.random_normal([config.vocab_size]), name='bias')\n",
    "        #outputs, status = rnn.static_rnn(lstm, inputs, sequence_length=lens)\n",
    "        #Dynamic RNN with given lengths for each poem\n",
    "        #status = initial_state\n",
    "        outputs, status = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state,sequence_length=lens,\n",
    "                                             dtype=tf.float32, scope='RNNLayer')\n",
    "        output = tf.reshape(outputs,[-1, config.embed_size])\n",
    "        logits = tf.matmul(output, weights) + biases\n",
    "        #logits = tf.contrib.layers.fully_connected(output, config.vocab_size, activation_fn=None)\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        \n",
    "\n",
    "    with tf.name_scope('Optimizer'):\n",
    "        targets = tf.reshape(output_targets, [-1])\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "            [logits],\n",
    "            [targets],\n",
    "            [tf.ones_like(targets, dtype=tf.float32)], config.vocab_size)\n",
    "        cost = tf.reduce_mean(loss) \n",
    "        learning_rate = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), 5)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    \n",
    "    with tf.name_scope('Validation'):\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        #Define initial state for validation set\n",
    "        initial_sample_state = cell.zero_state(1, tf.float32)\n",
    "        sample_input = tf.placeholder(tf.int32, shape=[1, 1])\n",
    "        sample_emd = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "        state_sample = initial_sample_state\n",
    "        output_sample, status_sample = tf.nn.dynamic_rnn(cell, sample_emd, initial_state=state_sample,\n",
    "                                             dtype=tf.float32, scope='RNNLayer')\n",
    "        output_sample = tf.reshape(output_sample,[-1, config.embed_size])\n",
    "        logits_sample = tf.matmul(output_sample, weights) + biases\n",
    "        #logits_sample = tf.contrib.layers.fully_connected(output_sample, config.vocab_size, activation_fn=None)\n",
    "        probs_sample= tf.nn.softmax(logits_sample)\n",
    "        prediction = tf.argmax(probs_sample, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.9042\n",
      "================================================================================\n",
      "(毕毕相相妻溅溅溅怅相相妻溅溅溅溅怅相相妻溅溅溅溅怅相相妻溅溅溅溅怅相相妻溅溅溅溅\n",
      "================================================================================\n",
      "Loss: 6.8254\n",
      "Loss: 5.9365\n",
      "================================================================================\n",
      "(今日日不不何，不不人不人人。)      ，不不不不人人。)      ，不不不\n",
      "================================================================================\n",
      "Loss: 5.8052\n",
      "Loss: 5.7192\n",
      "================================================================================\n",
      "(山山，不见人。)                                \n",
      "================================================================================\n",
      "Loss: 5.7306\n",
      "Loss: 5.6365\n",
      "================================================================================\n",
      "(日日人不知，风月山山人。)人不知见，不知见山。)人不知，不知见山。)人不知，不知\n",
      "================================================================================\n",
      "Loss: 5.5926\n",
      "Loss: 6.0964\n",
      "================================================================================\n",
      "(不知人知，不知不知人。)                            \n",
      "================================================================================\n",
      "Loss: 5.9374\n",
      "Loss: 5.8795\n",
      "================================================================================\n",
      "(白云风水中山，花花落日人间。)       ，不知人有人间。)       ，不\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "states = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for l in range(epochs):\n",
    "        for i in range(chunk_num):\n",
    "            sess.run(tf.assign(learning_rate, 0.002*((0.98)**l)))\n",
    "            x, y, lengths, _ = generatesamples.generate_batch()\n",
    "            feed_dict = {input_data:x, output_targets:y, lens:lengths}\n",
    "            l, _, _ = sess.run([cost, status, train_op], feed_dict=feed_dict)\n",
    "            if i%50 == 0:\n",
    "                print('Loss:', round(l, 4))\n",
    "            if i % 100 == 0:\n",
    "                print('=' * 80)\n",
    "                feed = np.zeros([1, 1])\n",
    "                feed[0, 0] = char2id('(')\n",
    "                s = id2char(feed[0][0])\n",
    "                #Initialize the original state\n",
    "                state_ = sess.run(initial_sample_state)\n",
    "                [pred_id, state_] = sess.run([prediction, status_sample], feed_dict={sample_input: feed, \n",
    "                                                                          state_sample: state_})\n",
    "            #pred_id = sess.run([prediction], {sample_input:feed})\n",
    "            #s = id2char(pred_id[0][0])\n",
    "            #生成10个单词\n",
    "                for _ in range(40):\n",
    "                    s += id2char(pred_id[0])\n",
    "                    feed[0, 0] = pred_id[0]\n",
    "                    [pred_id, state_] = sess.run([prediction, status_sample], feed_dict={sample_input: feed, \n",
    "                                                                       state_sample: state_})\n",
    "                print(s)\n",
    "                print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def lazy_property(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    vocab_size = len(vocab)\n",
    "    embed_size = 128\n",
    "    batch_size = 64\n",
    "    layer_size = 2\n",
    "    num_steps = 30\n",
    "    max_poem_len = max_poem_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class testConfig:\n",
    "    vocab_size = len(vocab)\n",
    "    embed_size = 128\n",
    "    batch_size = 1\n",
    "    num_steps = 1\n",
    "    layer_size = 2\n",
    "    max_poem_len = max_poem_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, in order to share parameters such as weights and biases in the architecture, it is necessary to specify \"variable_scope\" and use \"tf.get_variable\" method to share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Static RNN\n",
    "\n",
    "In this part, we will take all the peoms as a long text, and use previous character to predict next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define a RNN model \n",
    "class static_rnnModel:\n",
    "    '''Define a basic model for character classification, the model\n",
    "    Provides graph structure of tensorflow'''\n",
    "    \n",
    "    def __init__(self, x, y,  config, is_training=True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self._num_steps = config.num_steps\n",
    "        self.is_training = is_training\n",
    "        self.batch_size = config.batch_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embed_size = config.embed_size\n",
    "        self.learning_rate = None\n",
    "        self._initial_state = None\n",
    "        self._final_state = None\n",
    "        self.prediction\n",
    "        if not is_training:\n",
    "            print('Initializing Testing Model!')\n",
    "            return\n",
    "        self.cost\n",
    "        self.optimize\n",
    "        print('Initializing Training Model!') \n",
    "    \n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        #Calculatethe cost\n",
    "        logits= self.rnnLayer\n",
    "        #print(logits.shape)\n",
    "        targets = tf.reshape(self.y, [-1])\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "            [logits],\n",
    "            [targets],\n",
    "            [tf.ones_like(targets, dtype=tf.float32)])\n",
    "        cost = tf.reduce_mean(loss) \n",
    "        return cost\n",
    "\n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        cost = self.cost\n",
    "        #with tf.name_scope('Optimizer'):\n",
    "        self.learning_rate = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), 5)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        return train_op\n",
    "\n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        logits= self.rnnLayer\n",
    "        #print(logits.shape)\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        prediction = tf.argmax(probs, 1)\n",
    "        return prediction\n",
    "    \n",
    "    @lazy_property\n",
    "    def rnnLayer(self):\n",
    "        #Create two layers rnn\n",
    "        def lstm():\n",
    "            return rnn.BasicLSTMCell(self.embed_size, forget_bias=0.0, \n",
    "                                      state_is_tuple=True, reuse=tf.get_variable_scope().reuse) \n",
    "        lstm_cell = lstm\n",
    "        #if self.is_training:\n",
    "           #def lstm_cell():\n",
    "                #return tf.contrib.rnn.DropoutWrapper(lstm(), output_keep_prob=0.8)\n",
    "        cell = rnn.MultiRNNCell([lstm_cell() for _ in range(2)], state_is_tuple=True)\n",
    "            \n",
    "        self._initial_state = cell.zero_state(self.batch_size, tf.float32)\n",
    "        #Note we need get_variable method to share weights between training model and testing model\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embeddings = tf.get_variable('embedding2', [self.vocab_size,  self.embed_size])\n",
    "            inputs = tf.nn.embedding_lookup(embeddings, self.x)\n",
    "            \n",
    "        state = self._initial_state\n",
    "            \n",
    "        #inputs = tf.unstack(inputs, axis=1)\n",
    "        outputs = []\n",
    "        #Share the weights by specifyig variable scope\n",
    "        with tf.variable_scope('staticRNN'):      \n",
    "            for time_step in range(self._num_steps):\n",
    "                if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "                (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
    "                outputs.append(cell_output)\n",
    "        \n",
    "        self._final_state = state\n",
    "        output = tf.reshape(outputs,[-1, self.embed_size])\n",
    "        weights = tf.get_variable('weights', [self.embed_size, config.vocab_size], dtype=tf.float32)\n",
    "        biases = tf.get_variable('biases', [config.vocab_size], dtype=tf.float32)\n",
    "        logits = tf.matmul(output, weights) + biases\n",
    "        return logits\n",
    "    \n",
    "    def assign_lr(self, session, len_value):\n",
    "        session.run(self._update, feed_dict={self._new_len: len_value})\n",
    "    \n",
    "    @property\n",
    "    def learningRate(self):\n",
    "        return self.learning_rate\n",
    "    \n",
    "    def set_maxLen(self, max_len):\n",
    "        self._max_len = max_len\n",
    "    \n",
    "    @property\n",
    "    def initialState(self):\n",
    "        return self._initial_state\n",
    "    \n",
    "    @property\n",
    "    def finalState(self):\n",
    "        return self._final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Training Model!\n",
      "Initializing Testing Model!\n"
     ]
    }
   ],
   "source": [
    "graph2 = tf.Graph()\n",
    "\n",
    "with graph2.as_default():\n",
    "    initializer = tf.random_uniform_initializer(-0.05, 0.05)\n",
    "    with tf.name_scope('Train'):\n",
    "        input_data = tf.placeholder(tf.int32, [config.batch_size, None])\n",
    "        output_targets = tf.placeholder(tf.int32, [config.batch_size, None])\n",
    "        with tf.variable_scope(\"staticModel\", reuse=None, initializer=initializer):\n",
    "            train_model = static_rnnModel(input_data, output_targets, config)\n",
    "            \n",
    "    with tf.name_scope('Test'):\n",
    "        sample_input = tf.placeholder(tf.int32,[1, 1])\n",
    "        with tf.variable_scope(\"staticModel\", reuse=True, initializer=initializer):\n",
    "            test_model = static_rnnModel(sample_input, None, \n",
    "                                         testConfig, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.22\n",
      "(年年年年年公年年年公年年年公年年年公年年年公年年年公年年年公\n",
      "================================================================================\n",
      "Loss: 6.1621\n",
      "Loss: 6.1318\n",
      "(，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "================================================================================\n",
      "Loss: 6.127\n",
      "Loss: 6.12\n",
      "(，，，，，，，，，，，。。。。。。。。。。。。。。。。。。。\n",
      "================================================================================\n",
      "Loss: 6.0959\n",
      "Loss: 6.0506\n",
      "(，，，，，，，，，，，，，，，，，，。。。。。。。。。。。。\n",
      "================================================================================\n",
      "Loss: 5.991\n",
      "Loss: 5.9607\n",
      "()，，，。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "================================================================================\n",
      "Loss: 5.8693\n",
      "Loss: 5.8028\n",
      "(万，，，，，。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "================================================================================\n",
      "Loss: 5.6988\n",
      "(万，，，，，。。。。。。。。。。。。。，，，，，，，，，，，\n",
      "================================================================================\n",
      "Loss: 5.5988\n",
      "Loss: 5.4299\n",
      "(万。。。，，，。。。。。。。。。。。。。。。。。。。。。，，\n",
      "================================================================================\n",
      "Loss: 5.2344\n",
      "Loss: 5.0437\n",
      "(万。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "================================================================================\n",
      "Loss: 4.8609\n",
      "Loss: 4.6884\n",
      "(万。。君。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "================================================================================\n",
      "Loss: 4.5264\n",
      "Loss: 4.3674\n",
      "(万。君君君。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "================================================================================\n",
      "Loss: 4.2115\n",
      "Loss: 4.0547\n",
      "(万。君君君君。((。。。。((。。。。。。。。。。。。。。。\n",
      "================================================================================\n",
      "Loss: 4.2219\n",
      "(万。君君君君。((((((((((((。。。。。。。。。。。\n",
      "================================================================================\n",
      "Loss: 4.0747\n",
      "Loss: 3.9313\n",
      "(万光君君君君。(((((((((((((((((((((((\n",
      "================================================================================\n",
      "Loss: 3.7739\n",
      "Loss: 3.6179\n",
      "(万光君君君君君(((((((((((((((((((((((\n",
      "================================================================================\n",
      "Loss: 3.4698\n",
      "Loss: 3.2868\n",
      "(万光君君君君。((((((((((((((。。，，，，，，，\n",
      "================================================================================\n",
      "Loss: 3.1141\n",
      "Loss: 2.9504\n",
      "(万光君君君君((末。。((((，，，，，。。。。。。。。。。\n",
      "================================================================================\n",
      "Loss: 2.781\n",
      "Loss: 2.6208\n",
      "(万光君君君((云末，恐深 ，，杂，，，，，，，，，，，，，，\n",
      "================================================================================\n",
      "Loss: 2.2223\n",
      "(万光君君龙君竹良官，(((，，，，，，，，，，，，，，，，，\n",
      "================================================================================\n",
      "Loss: 2.0827\n",
      "Loss: 1.958\n",
      "(万光光君君光光(((湖湖湖湖空肉空。((沉沉。，，，。公白。\n",
      "================================================================================\n",
      "Loss: 1.8461\n",
      "Loss: 1.725\n",
      "(万光光君君光(((湖湖湖空吹五青遗敢祚击祚影击祚祚影击北击击\n",
      "================================================================================\n",
      "Loss: 1.6242\n",
      "Loss: 1.5205\n",
      "(万光光君君光(((湖湖湖湖别沉沉沉沉沉公向风)。。。)。。。\n",
      "================================================================================\n",
      "Loss: 1.4255\n",
      "Loss: 1.3386\n",
      "(万光光君君((((湖遇白。归，，，，，，，，，，，，，，，，\n",
      "================================================================================\n",
      "Loss: 1.2576\n",
      "Loss: 1.1731\n",
      "(万光光君光光取投萼臣急路路(((((钱。。。，，，，，，，，\n",
      "================================================================================\n",
      "Loss: 1.2377\n",
      "(万光光君光光取弹携峰灵此竟。路谁路路路路草遗可。。。。，，，\n",
      "================================================================================\n",
      "Loss: 1.1605\n",
      "Loss: 1.0992\n",
      "(万光龙君小老日，公雨。。。。。。。。。。。。。。。。。。。。\n",
      "================================================================================\n",
      "Loss: 1.0084\n",
      "Loss: 0.9402\n",
      "(万光龙君小难(寒日，沉公石日，(。。。。。。。。。。。。。。\n",
      "================================================================================\n",
      "Loss: 0.8734\n",
      "Loss: 0.8118\n",
      "(万光光君小((提空泉街韵偏故成逢  乡潺旧旧贫贫缑罗国数寻，\n",
      "================================================================================\n",
      "Loss: 0.7538\n",
      "Loss: 0.7116\n",
      "(万光光君小((提空日，空饵门生长。。。。。。。。。。。。。。\n",
      "================================================================================\n",
      "Loss: 0.6456\n",
      "Loss: 0.5991\n",
      "(万光君君君(云云微日春春云云云举云自举闻云春，三轻。。)))\n",
      "================================================================================\n",
      "Loss: 0.5025\n",
      "(万光君君君((云微日春春云举云自举闻云春天，，三轻。))。。\n",
      "================================================================================\n",
      "Loss: 0.4629\n",
      "Loss: 0.4312\n",
      "(万光光君小((((沉沉此))晓)))))))))))))))\n",
      "================================================================================\n",
      "Loss: 0.4043\n",
      "Loss: 0.3756\n",
      "(万光龙君小(((难风晓)))落草))))))))))))))\n",
      "================================================================================\n",
      "Loss: 0.3496\n",
      "Loss: 0.3285\n",
      "(万光龙君小(((难风落晓))心石日，))，，，。。。。。。。\n",
      "================================================================================\n",
      "Loss: 0.3089\n",
      "Loss: 0.2878\n",
      "()(还还。。来虽隐应成成成成成骨羊与方添，赏重人赏重人赏重人\n",
      "================================================================================\n",
      "Loss: 0.2766\n",
      "Loss: 0.2528\n",
      "()(还还。。来虽应。营幽街旅 日，夫。。，。，，，，，，，，\n",
      "================================================================================\n",
      "Loss: 0.2625\n",
      "(万光龙君光(((难风晓晓铸事老落洼重风来歌草草丧说草骝润别别\n",
      "================================================================================\n",
      "Loss: 0.2465\n",
      "Loss: 0.2311\n",
      "(万光光迹且归发向晓风风香日，风，来歌士。后))疑)少))来崇\n",
      "================================================================================\n",
      "Loss: 0.2168\n",
      "Loss: 0.2039\n",
      "(万光光迹且归发向晓风风香日，风风。香雪风风来歌士。后))))\n",
      "================================================================================\n",
      "Loss: 0.193\n",
      "Loss: 0.1815\n",
      "(万光光迹且归发向晓风风来歌士。后后后不。少草草))逢)来煮。\n",
      "================================================================================\n",
      "Loss: 0.1715\n",
      "Loss: 0.224\n",
      "(筑半发发发发发发发发发发发发发发发扉隐移移空击击击击击击击击\n",
      "================================================================================\n",
      "Loss: 0.1589\n",
      "Loss: 0.1478\n",
      "()(还还。。来虽应。营今可)，))))))))))。白。。历\n",
      "================================================================================\n",
      "Loss: 0.129\n",
      "()(还还。。来虽应。营今可)，))))))))))))。白。\n",
      "================================================================================\n",
      "Loss: 0.1226\n",
      "Loss: 0.1167\n",
      "()(还还。不更见。。。。。。。。。，，，，，，，，，，，，，\n",
      "================================================================================\n",
      "Loss: 0.1115\n",
      "Loss: 0.1065\n",
      "()(还还。不更见。。。。，。。。。，，，，，，，，，，，，，\n",
      "================================================================================\n",
      "Loss: 0.102\n",
      "Loss: 0.0975\n",
      "()(还还。不更见。。。。，，。。。。，，，，，，，，，，，，\n",
      "================================================================================\n",
      "Loss: 0.0934\n",
      "Loss: 0.0901\n",
      "()(还还。。世世世世我。转身江肉江((江相游游游峰(公求。忘\n",
      "================================================================================\n",
      "Loss: 0.0858\n",
      "Loss: 0.0821\n",
      "()(还还。不更见。夹更更见。不。难。香雪少逢)。来歌还还。夫\n",
      "================================================================================\n",
      "Loss: 0.0817\n",
      "(万光龙君迹难难难来歌草草洼重，来歌士。后后后后后后后后后后后\n",
      "================================================================================\n",
      "Loss: 0.0785\n",
      "Loss: 0.0754\n",
      "(万光龙君迹难难难来歌草草洼重乡雪风来煮。家，，，，，雪家，今\n",
      "================================================================================\n",
      "Loss: 0.0725\n",
      "Loss: 0.0696\n",
      "(万光龙君迹难难难来歌草草洼重乡雪风来煮。家，，，，，雪家，今\n",
      "================================================================================\n",
      "Loss: 0.067\n",
      "Loss: 0.0643\n",
      "(万光龙迹且归分魂向风来歌草山乡碧上上来筹后后后后，(亦亦张来\n",
      "================================================================================\n",
      "Loss: 0.062\n",
      "Loss: 0.0596\n",
      "(万光龙迹且归分魂向雁可无来 下下，顿，城家家老。。不。张来头\n",
      "================================================================================\n",
      "Loss: 0.0575\n",
      "Loss: 0.0554\n",
      "(万光龙迹且归分魂向雁可无来可无来日。卿。来盏营见摇(，，，，\n",
      "================================================================================\n",
      "Loss: 0.0543\n",
      "()半心尽日，少殿本此长应。海侯纷长)，长。白。。受，白。。盘\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-8aa782929ad8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m#train_model.set_maxLen(max_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/richardsun/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/richardsun/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/richardsun/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/richardsun/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/richardsun/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "states = []\n",
    "with tf.Session(graph=graph2) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    #record initial state\n",
    "    for l in range(epochs):\n",
    "        sess.run(tf.assign(train_model.learningRate, 0.002*(0.97**l)))\n",
    "        for i in range(chunk_num):\n",
    "            #Assign a learning rate\n",
    "            #sess.run(tf.assign(model.isTraining, True))\n",
    "            x, y = generatesamples.generate_batch_same_size(config.num_steps)\n",
    "            feed_dict = {input_data:x, output_targets:y}\n",
    "            #train_model.set_maxLen(max_len)\n",
    "            l,  _ = sess.run([train_model.cost, train_model.optimize], feed_dict)\n",
    "            if i%50 == 0:\n",
    "                print('Loss:', round(l, 4))\n",
    "            if i%100 == 0:\n",
    "                #Initialize testing input\n",
    "                feed = np.zeros([1, 1])\n",
    "                feed[0, 0] = char2id('(')\n",
    "                s = '('\n",
    "                #Specify initial state\n",
    "                #Initialize the state of testing data\n",
    "                initial_state = sess.run(test_model.initialState)\n",
    "                state = initial_state\n",
    "\n",
    "                #生成10个单词\n",
    "                for _ in range(30):\n",
    "                    fd= {sample_input:feed, test_model.initialState:state}\n",
    "                    state, pred_id = sess.run([test_model.finalState, \n",
    "                                                     test_model.prediction], feed_dict=fd)\n",
    "                    #feed = np.full((1,testConfig.max_poem_len), char2id(' '), np.int32)\n",
    "                    feed[0, 0] = pred_id[0]\n",
    "                    s += id2char(feed[0, 0])\n",
    "                print(s)\n",
    "                print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
